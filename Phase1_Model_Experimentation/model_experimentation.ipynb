{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1/3: Team Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (34272, 207)\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    with h5py.File(r'C:\\foai\\94879-starter-code-Team-Project\\metr-la.h5', 'r') as f:\n",
    "        df_group = f['df']\n",
    "        \n",
    "        # Load the column names (features) from 'block0_items'\n",
    "        columns = list(df_group['block0_items'][:].astype(str))\n",
    "        \n",
    "        # Load the actual data from 'block0_values'\n",
    "        data = df_group['block0_values'][:]\n",
    "        \n",
    "        # Convert the data into a DataFrame\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Load the data and inspect the first few rows\n",
    "df = load_data()\n",
    "print(\"Data shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install Flask==2.0.1 \n",
    "conda install Werkzeug==2.0.1 \n",
    "conda install tensorflow==2.6.0 \n",
    "conda install numpy==1.19.5 \n",
    "conda install scikit-learn==0.24.2 \n",
    "conda install joblib==1.0.1\n",
    "pip install mlflow==1.20.2 \n",
    "conda install pandas==1.3.3 \n",
    "pip install protobuf==3.18.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys inside 'df' group: ['axis0', 'axis1', 'block0_items', 'block0_values']\n"
     ]
    }
   ],
   "source": [
    "# Load and inspect the contents of the 'df' group\n",
    "def load_data():\n",
    "    with h5py.File(r'C:\\foai\\94879-starter-code-Team-Project\\metr-la.h5', 'r') as f:\n",
    "        df_group = f['df']\n",
    "        \n",
    "        # List all keys within the 'df' group\n",
    "        print(\"Keys inside 'df' group:\", list(df_group.keys()))\n",
    "        \n",
    "        # You can then load individual datasets once you know the keys\n",
    "        # For example, if a key 'data' exists:\n",
    "        if 'data' in df_group:\n",
    "            data = df_group['data'][:]\n",
    "            print(\"Loaded data shape:\", data.shape)\n",
    "        return df_group\n",
    "\n",
    "# Inspect the 'df' group\n",
    "df_group = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (34272, 207)\n"
     ]
    }
   ],
   "source": [
    "# Load and reconstruct the DataFrame from the 'df' group\n",
    "def load_data():\n",
    "    with h5py.File(r'C:\\foai\\94879-starter-code-Team-Project\\metr-la.h5', 'r') as f:\n",
    "        df_group = f['df']\n",
    "        \n",
    "        # Load the column names (features) from 'block0_items'\n",
    "        columns = list(df_group['block0_items'][:].astype(str))\n",
    "        \n",
    "        # Load the actual data from 'block0_values'\n",
    "        data = df_group['block0_values'][:]\n",
    "        \n",
    "        # Convert the data into a DataFrame\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Load the data and inspect the first few rows\n",
    "df = load_data()\n",
    "print(\"Data shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>773869</th>\n",
       "      <th>767541</th>\n",
       "      <th>767542</th>\n",
       "      <th>717447</th>\n",
       "      <th>717446</th>\n",
       "      <th>717445</th>\n",
       "      <th>773062</th>\n",
       "      <th>767620</th>\n",
       "      <th>737529</th>\n",
       "      <th>717816</th>\n",
       "      <th>...</th>\n",
       "      <th>772167</th>\n",
       "      <th>769372</th>\n",
       "      <th>774204</th>\n",
       "      <th>769806</th>\n",
       "      <th>717590</th>\n",
       "      <th>717592</th>\n",
       "      <th>717595</th>\n",
       "      <th>772168</th>\n",
       "      <th>718141</th>\n",
       "      <th>769373</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.375000</td>\n",
       "      <td>67.625000</td>\n",
       "      <td>67.125000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>66.875000</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>65.125000</td>\n",
       "      <td>67.125000</td>\n",
       "      <td>59.625000</td>\n",
       "      <td>62.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>45.625000</td>\n",
       "      <td>65.500</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>66.428571</td>\n",
       "      <td>66.875</td>\n",
       "      <td>59.375000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>61.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62.666667</td>\n",
       "      <td>68.555556</td>\n",
       "      <td>65.444444</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>68.111111</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>57.444444</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>50.666667</td>\n",
       "      <td>69.875</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>58.555556</td>\n",
       "      <td>62.000</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>55.888889</td>\n",
       "      <td>68.444444</td>\n",
       "      <td>62.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>64.250000</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>68.125</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>61.375000</td>\n",
       "      <td>69.857143</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57.333333</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>67.666667</td>\n",
       "      <td>61.666667</td>\n",
       "      <td>67.333333</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>60.666667</td>\n",
       "      <td>67.333333</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>70.000</td>\n",
       "      <td>68.333333</td>\n",
       "      <td>57.333333</td>\n",
       "      <td>66.000</td>\n",
       "      <td>54.666667</td>\n",
       "      <td>64.666667</td>\n",
       "      <td>57.666667</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>57.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66.500000</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>67.875000</td>\n",
       "      <td>62.375000</td>\n",
       "      <td>64.375000</td>\n",
       "      <td>67.750000</td>\n",
       "      <td>65.125000</td>\n",
       "      <td>64.875000</td>\n",
       "      <td>56.250000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>41.250000</td>\n",
       "      <td>69.375</td>\n",
       "      <td>59.500000</td>\n",
       "      <td>44.625000</td>\n",
       "      <td>64.250</td>\n",
       "      <td>62.625000</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>69.375000</td>\n",
       "      <td>61.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63.625000</td>\n",
       "      <td>67.250000</td>\n",
       "      <td>63.250000</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>57.375000</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>64.625000</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>60.375000</td>\n",
       "      <td>66.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>65.875</td>\n",
       "      <td>59.750000</td>\n",
       "      <td>64.125000</td>\n",
       "      <td>66.125</td>\n",
       "      <td>62.375000</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>68.625000</td>\n",
       "      <td>59.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68.750000</td>\n",
       "      <td>65.250000</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>65.125000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>65.125000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>62.625000</td>\n",
       "      <td>66.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>68.375</td>\n",
       "      <td>61.250000</td>\n",
       "      <td>64.375000</td>\n",
       "      <td>66.500</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>64.375000</td>\n",
       "      <td>48.625000</td>\n",
       "      <td>67.625000</td>\n",
       "      <td>61.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>63.500000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>58.125000</td>\n",
       "      <td>66.625000</td>\n",
       "      <td>64.250000</td>\n",
       "      <td>64.875000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>53.250000</td>\n",
       "      <td>60.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>41.375000</td>\n",
       "      <td>69.250</td>\n",
       "      <td>62.625000</td>\n",
       "      <td>58.875000</td>\n",
       "      <td>61.125</td>\n",
       "      <td>64.250000</td>\n",
       "      <td>66.125000</td>\n",
       "      <td>50.750000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>62.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      773869     767541     767542     717447     717446     717445  \\\n",
       "0  64.375000  67.625000  67.125000  61.500000  66.875000  68.750000   \n",
       "1  62.666667  68.555556  65.444444  62.444444  64.444444  68.111111   \n",
       "2  64.000000  63.750000  60.000000  59.000000  66.500000  66.250000   \n",
       "3   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5  57.333333  69.000000  67.666667  61.666667  67.333333  69.000000   \n",
       "6  66.500000  63.875000  67.875000  62.375000  64.375000  67.750000   \n",
       "7  63.625000  67.250000  63.250000  60.500000  57.375000  65.500000   \n",
       "8  68.750000  65.250000  63.500000  63.000000  65.125000  68.000000   \n",
       "9  63.500000  61.500000  62.500000  58.125000  66.625000  64.250000   \n",
       "\n",
       "      773062     767620     737529     717816  ...     772167  769372  \\\n",
       "0  65.125000  67.125000  59.625000  62.750000  ...  45.625000  65.500   \n",
       "1  65.000000  65.000000  57.444444  63.333333  ...  50.666667  69.875   \n",
       "2  64.500000  64.250000  63.875000  65.375000  ...  44.125000  69.000   \n",
       "3   0.000000   0.000000   0.000000   0.000000  ...   0.000000   0.000   \n",
       "4   0.000000   0.000000   0.000000   0.000000  ...   0.000000   0.000   \n",
       "5  60.666667  67.333333  63.000000  63.333333  ...  42.000000  70.000   \n",
       "6  65.125000  64.875000  56.250000  63.000000  ...  41.250000  69.375   \n",
       "7  64.625000  65.500000  60.375000  66.625000  ...  52.000000  65.875   \n",
       "8  65.125000  63.750000  62.625000  66.750000  ...  52.500000  68.375   \n",
       "9  64.875000  66.500000  53.250000  60.750000  ...  41.375000  69.250   \n",
       "\n",
       "      774204     769806  717590     717592     717595     772168     718141  \\\n",
       "0  64.500000  66.428571  66.875  59.375000  69.000000  59.250000  69.000000   \n",
       "1  66.666667  58.555556  62.000  61.111111  64.444444  55.888889  68.444444   \n",
       "2  56.500000  59.250000  68.125  62.500000  65.625000  61.375000  69.857143   \n",
       "3   0.000000   0.000000   0.000   0.000000   0.000000   0.000000   0.000000   \n",
       "4   0.000000   0.000000   0.000   0.000000   0.000000   0.000000   0.000000   \n",
       "5  68.333333  57.333333  66.000  54.666667  64.666667  57.666667  69.000000   \n",
       "6  59.500000  44.625000  64.250  62.625000  65.500000  51.000000  69.375000   \n",
       "7  59.750000  64.125000  66.125  62.375000  67.500000  52.000000  68.625000   \n",
       "8  61.250000  64.375000  66.500  66.250000  64.375000  48.625000  67.625000   \n",
       "9  62.625000  58.875000  61.125  64.250000  66.125000  50.750000  66.250000   \n",
       "\n",
       "      769373  \n",
       "0  61.875000  \n",
       "1  62.875000  \n",
       "2  62.000000  \n",
       "3   0.000000  \n",
       "4   0.000000  \n",
       "5  57.333333  \n",
       "6  61.250000  \n",
       "7  59.375000  \n",
       "8  61.750000  \n",
       "9  62.250000  \n",
       "\n",
       "[10 rows x 207 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axis0 data (row indices): [b'773869' b'767541' b'767542' b'717447' b'717446' b'717445' b'773062'\n",
      " b'767620' b'737529' b'717816' b'765604' b'767471' b'716339' b'773906'\n",
      " b'765273' b'716331' b'771667' b'716337' b'769953' b'769402' b'769403'\n",
      " b'769819' b'769405' b'716941' b'717578' b'716960' b'717804' b'767572'\n",
      " b'767573' b'773012' b'773013' b'764424' b'769388' b'716328' b'717819'\n",
      " b'769941' b'760987' b'718204' b'718045' b'769418' b'768066' b'772140'\n",
      " b'773927' b'760024' b'774012' b'774011' b'767609' b'769359' b'760650'\n",
      " b'716956' b'769831' b'761604' b'717495' b'716554' b'773953' b'767470'\n",
      " b'716955' b'764949' b'773954' b'767366' b'769444' b'773939' b'774067'\n",
      " b'769443' b'767750' b'767751' b'767610' b'773880' b'764766' b'717497'\n",
      " b'717490' b'717491' b'717492' b'717493' b'765176' b'717498' b'717499'\n",
      " b'765171' b'718064' b'718066' b'765164' b'769431' b'769430' b'717610'\n",
      " b'767053' b'767621' b'772596' b'772597' b'767350' b'767351' b'716571'\n",
      " b'773023' b'767585' b'773024' b'717483' b'718379' b'717481' b'717480'\n",
      " b'717486' b'764120' b'772151' b'718371' b'717489' b'717488' b'717818'\n",
      " b'718076' b'718072' b'767455' b'767454' b'761599' b'717099' b'773916'\n",
      " b'716968' b'769467' b'717576' b'717573' b'717572' b'717571' b'717570'\n",
      " b'764760' b'718089' b'769847' b'717608' b'767523' b'716942' b'718090'\n",
      " b'769867' b'717472' b'717473' b'759591' b'764781' b'765099' b'762329'\n",
      " b'716953' b'716951' b'767509' b'765182' b'769358' b'772513' b'716958'\n",
      " b'718496' b'769346' b'773904' b'718499' b'764853' b'761003' b'717502'\n",
      " b'759602' b'717504' b'763995' b'717508' b'765265' b'773996' b'773995'\n",
      " b'717469' b'717468' b'764106' b'717465' b'764794' b'717466' b'717461'\n",
      " b'717460' b'717463' b'717462' b'769345' b'716943' b'772669' b'717582'\n",
      " b'717583' b'717580' b'716949' b'717587' b'772178' b'717585' b'716939'\n",
      " b'768469' b'764101' b'767554' b'773975' b'773974' b'717510' b'717513'\n",
      " b'717825' b'767495' b'767494' b'717821' b'717823' b'717458' b'717459'\n",
      " b'769926' b'764858' b'717450' b'717452' b'717453' b'759772' b'717456'\n",
      " b'771673' b'772167' b'769372' b'774204' b'769806' b'717590' b'717592'\n",
      " b'717595' b'772168' b'718141' b'769373']\n",
      "axis1 data (column indices): [1330560000000000000 1330560300000000000 1330560600000000000 ...\n",
      " 1340840700000000000 1340841000000000000 1340841300000000000]\n",
      "block0_items (column labels): ['773869' '767541' '767542' '717447' '717446' '717445' '773062' '767620'\n",
      " '737529' '717816' '765604' '767471' '716339' '773906' '765273' '716331'\n",
      " '771667' '716337' '769953' '769402' '769403' '769819' '769405' '716941'\n",
      " '717578' '716960' '717804' '767572' '767573' '773012' '773013' '764424'\n",
      " '769388' '716328' '717819' '769941' '760987' '718204' '718045' '769418'\n",
      " '768066' '772140' '773927' '760024' '774012' '774011' '767609' '769359'\n",
      " '760650' '716956' '769831' '761604' '717495' '716554' '773953' '767470'\n",
      " '716955' '764949' '773954' '767366' '769444' '773939' '774067' '769443'\n",
      " '767750' '767751' '767610' '773880' '764766' '717497' '717490' '717491'\n",
      " '717492' '717493' '765176' '717498' '717499' '765171' '718064' '718066'\n",
      " '765164' '769431' '769430' '717610' '767053' '767621' '772596' '772597'\n",
      " '767350' '767351' '716571' '773023' '767585' '773024' '717483' '718379'\n",
      " '717481' '717480' '717486' '764120' '772151' '718371' '717489' '717488'\n",
      " '717818' '718076' '718072' '767455' '767454' '761599' '717099' '773916'\n",
      " '716968' '769467' '717576' '717573' '717572' '717571' '717570' '764760'\n",
      " '718089' '769847' '717608' '767523' '716942' '718090' '769867' '717472'\n",
      " '717473' '759591' '764781' '765099' '762329' '716953' '716951' '767509'\n",
      " '765182' '769358' '772513' '716958' '718496' '769346' '773904' '718499'\n",
      " '764853' '761003' '717502' '759602' '717504' '763995' '717508' '765265'\n",
      " '773996' '773995' '717469' '717468' '764106' '717465' '764794' '717466'\n",
      " '717461' '717460' '717463' '717462' '769345' '716943' '772669' '717582'\n",
      " '717583' '717580' '716949' '717587' '772178' '717585' '716939' '768469'\n",
      " '764101' '767554' '773975' '773974' '717510' '717513' '717825' '767495'\n",
      " '767494' '717821' '717823' '717458' '717459' '769926' '764858' '717450'\n",
      " '717452' '717453' '759772' '717456' '771673' '772167' '769372' '774204'\n",
      " '769806' '717590' '717592' '717595' '772168' '718141' '769373']\n",
      "block0_values shape: (34272, 207)\n",
      "Sample data from block0_values: [[64.375      67.625      67.125      ... 59.25       69.\n",
      "  61.875     ]\n",
      " [62.66666667 68.55555556 65.44444444 ... 55.88888889 68.44444444\n",
      "  62.875     ]\n",
      " [64.         63.75       60.         ... 61.375      69.85714286\n",
      "  62.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# File path to the HDF5 data\n",
    "file_path = r'C:\\foai\\94879-starter-code-Team-Project\\metr-la.h5'\n",
    "\n",
    "# Load the data using h5py and inspect each key\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    df_group = file['df']\n",
    "    \n",
    "    # Inspecting 'axis0'\n",
    "    axis0_data = df_group['axis0'][:]\n",
    "    print(\"axis0 data (row indices):\", axis0_data)\n",
    "\n",
    "    # Inspecting 'axis1'\n",
    "    axis1_data = df_group['axis1'][:]\n",
    "    print(\"axis1 data (column indices):\", axis1_data)\n",
    "\n",
    "    # Inspecting 'block0_items' (column labels)\n",
    "    block0_items = df_group['block0_items'][:].astype(str)\n",
    "    print(\"block0_items (column labels):\", block0_items)\n",
    "\n",
    "    # Inspecting 'block0_values' (data values)\n",
    "    block0_values = df_group['block0_values'][:]\n",
    "    print(\"block0_values shape:\", block0_values.shape)\n",
    "    print(\"Sample data from block0_values:\", block0_values[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Checking for Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for missing values...\n",
      "773869    0\n",
      "767541    0\n",
      "767542    0\n",
      "717447    0\n",
      "717446    0\n",
      "         ..\n",
      "717592    0\n",
      "717595    0\n",
      "772168    0\n",
      "718141    0\n",
      "769373    0\n",
      "Length: 207, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for missing values...\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Checking df Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>773869</th>\n",
       "      <th>767541</th>\n",
       "      <th>767542</th>\n",
       "      <th>717447</th>\n",
       "      <th>717446</th>\n",
       "      <th>717445</th>\n",
       "      <th>773062</th>\n",
       "      <th>767620</th>\n",
       "      <th>737529</th>\n",
       "      <th>717816</th>\n",
       "      <th>...</th>\n",
       "      <th>772167</th>\n",
       "      <th>769372</th>\n",
       "      <th>774204</th>\n",
       "      <th>769806</th>\n",
       "      <th>717590</th>\n",
       "      <th>717592</th>\n",
       "      <th>717595</th>\n",
       "      <th>772168</th>\n",
       "      <th>718141</th>\n",
       "      <th>769373</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "      <td>34272.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.631359</td>\n",
       "      <td>60.452789</td>\n",
       "      <td>60.726120</td>\n",
       "      <td>49.524287</td>\n",
       "      <td>46.079798</td>\n",
       "      <td>50.952003</td>\n",
       "      <td>54.471684</td>\n",
       "      <td>57.255095</td>\n",
       "      <td>56.068044</td>\n",
       "      <td>52.871841</td>\n",
       "      <td>...</td>\n",
       "      <td>37.803342</td>\n",
       "      <td>58.156679</td>\n",
       "      <td>51.217523</td>\n",
       "      <td>59.795754</td>\n",
       "      <td>59.329923</td>\n",
       "      <td>56.915083</td>\n",
       "      <td>62.484679</td>\n",
       "      <td>54.697381</td>\n",
       "      <td>58.920210</td>\n",
       "      <td>51.197504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22.619199</td>\n",
       "      <td>15.970239</td>\n",
       "      <td>18.313353</td>\n",
       "      <td>15.843261</td>\n",
       "      <td>19.350345</td>\n",
       "      <td>16.681760</td>\n",
       "      <td>17.984761</td>\n",
       "      <td>18.751065</td>\n",
       "      <td>18.240361</td>\n",
       "      <td>23.343805</td>\n",
       "      <td>...</td>\n",
       "      <td>13.525743</td>\n",
       "      <td>20.690411</td>\n",
       "      <td>22.224997</td>\n",
       "      <td>16.126225</td>\n",
       "      <td>19.849950</td>\n",
       "      <td>18.260438</td>\n",
       "      <td>16.959238</td>\n",
       "      <td>16.303651</td>\n",
       "      <td>19.080474</td>\n",
       "      <td>21.239354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>60.364583</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>65.444444</td>\n",
       "      <td>50.333333</td>\n",
       "      <td>34.666667</td>\n",
       "      <td>49.555556</td>\n",
       "      <td>55.750000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>58.222222</td>\n",
       "      <td>43.428571</td>\n",
       "      <td>...</td>\n",
       "      <td>30.444444</td>\n",
       "      <td>64.111111</td>\n",
       "      <td>53.444444</td>\n",
       "      <td>61.714286</td>\n",
       "      <td>63.666667</td>\n",
       "      <td>60.222222</td>\n",
       "      <td>65.888889</td>\n",
       "      <td>50.125000</td>\n",
       "      <td>62.888889</td>\n",
       "      <td>54.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>64.888889</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>67.375000</td>\n",
       "      <td>53.875000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>56.111111</td>\n",
       "      <td>62.111111</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>65.875000</td>\n",
       "      <td>...</td>\n",
       "      <td>43.222222</td>\n",
       "      <td>67.111111</td>\n",
       "      <td>61.777778</td>\n",
       "      <td>64.875000</td>\n",
       "      <td>66.777778</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>67.625000</td>\n",
       "      <td>61.125000</td>\n",
       "      <td>66.125000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>66.875000</td>\n",
       "      <td>66.375000</td>\n",
       "      <td>68.444444</td>\n",
       "      <td>58.125000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>60.333333</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>64.888889</td>\n",
       "      <td>67.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>46.625000</td>\n",
       "      <td>68.444444</td>\n",
       "      <td>64.375000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>68.250000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>68.625000</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>67.750000</td>\n",
       "      <td>63.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             773869        767541        767542        717447        717446  \\\n",
       "count  34272.000000  34272.000000  34272.000000  34272.000000  34272.000000   \n",
       "mean      54.631359     60.452789     60.726120     49.524287     46.079798   \n",
       "std       22.619199     15.970239     18.313353     15.843261     19.350345   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       60.364583     63.000000     65.444444     50.333333     34.666667   \n",
       "50%       64.888889     65.000000     67.375000     53.875000     46.000000   \n",
       "75%       66.875000     66.375000     68.444444     58.125000     64.500000   \n",
       "max       70.000000     70.000000     70.000000     70.000000     70.000000   \n",
       "\n",
       "             717445        773062        767620        737529        717816  \\\n",
       "count  34272.000000  34272.000000  34272.000000  34272.000000  34272.000000   \n",
       "mean      50.952003     54.471684     57.255095     56.068044     52.871841   \n",
       "std       16.681760     17.984761     18.751065     18.240361     23.343805   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       49.555556     55.750000     61.000000     58.222222     43.428571   \n",
       "50%       56.111111     62.111111     63.333333     62.444444     65.875000   \n",
       "75%       60.333333     65.000000     65.000000     64.888889     67.625000   \n",
       "max       70.000000     70.000000     70.000000     70.000000     70.000000   \n",
       "\n",
       "       ...        772167        769372        774204        769806  \\\n",
       "count  ...  34272.000000  34272.000000  34272.000000  34272.000000   \n",
       "mean   ...     37.803342     58.156679     51.217523     59.795754   \n",
       "std    ...     13.525743     20.690411     22.224997     16.126225   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...     30.444444     64.111111     53.444444     61.714286   \n",
       "50%    ...     43.222222     67.111111     61.777778     64.875000   \n",
       "75%    ...     46.625000     68.444444     64.375000     66.500000   \n",
       "max    ...     65.000000     70.000000     70.000000     70.000000   \n",
       "\n",
       "             717590        717592        717595        772168        718141  \\\n",
       "count  34272.000000  34272.000000  34272.000000  34272.000000  34272.000000   \n",
       "mean      59.329923     56.915083     62.484679     54.697381     58.920210   \n",
       "std       19.849950     18.260438     16.959238     16.303651     19.080474   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       63.666667     60.222222     65.888889     50.125000     62.888889   \n",
       "50%       66.777778     63.000000     67.625000     61.125000     66.125000   \n",
       "75%       68.250000     64.750000     68.625000     64.444444     67.750000   \n",
       "max       70.000000     70.000000     70.000000     70.000000     70.000000   \n",
       "\n",
       "             769373  \n",
       "count  34272.000000  \n",
       "mean      51.197504  \n",
       "std       21.239354  \n",
       "min        0.000000  \n",
       "25%       54.125000  \n",
       "50%       62.000000  \n",
       "75%       63.444444  \n",
       "max       70.000000  \n",
       "\n",
       "[8 rows x 207 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nStatistical summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Feature Engineering (BEFORE THE PROCESSING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after feature engineering: (34270, 621)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering (optimized to avoid fragmentation)\n",
    "def feature_engineering(df):\n",
    "    # Compute rolling mean and std for each sensor at once using apply\n",
    "    rolling_means = df.rolling(window=3).mean().add_suffix('_rolling_mean')\n",
    "    rolling_stds = df.rolling(window=3).std().add_suffix('_rolling_std')\n",
    "\n",
    "    # Combine the original dataframe with the new rolling features\n",
    "    df_combined = pd.concat([df, rolling_means, rolling_stds], axis=1)\n",
    "\n",
    "    # Drop NaN values that were introduced by the rolling window\n",
    "    df_combined = df_combined.dropna()\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "# Apply feature engineering\n",
    "df = feature_engineering(df)\n",
    "\n",
    "# Check the updated dataframe\n",
    "print(f\"Data shape after feature engineering: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Using MinMaxScaler to Preprocess Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    return data_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the entire feature-engineered dataset\n",
    "data_scaled, scaler = preprocess_data(df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Sequences for Time-Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (27408, 10, 621), y_train shape: (27408, 1)\n",
      "X_test shape: (6852, 10, 621), y_test shape: (6852, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 2: Create sequences from the scaled data\n",
    "def create_sequences(data, time_steps=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])  # Sequence of features\n",
    "        y.append(data[i + time_steps, 0])  # Predict the next value for the first feature\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Apply preprocessing and sequence creation\n",
    "data_scaled, scaler = preprocess_data(df.values)\n",
    "X, y = create_sequences(data_scaled, time_steps=10)\n",
    "\n",
    "# Step 3: Split into train and test\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Ensure y_train and y_test are 2D arrays\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save the Preprocessed Data (Pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preprocessed_data.pkl']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((X, y), 'preprocessed_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "\n",
    "print(kfp.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Building LSTM & GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1: LSTM Model Architecture\n",
    "##### Just for reference. Model being built in `train_LSTM.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(y_train.shape[1]))  \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1: GRU Model Architecture\n",
    "##### Just for reference. Model being built in `train_GRU.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(64, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(GRU(32))\n",
    "    model.add(Dense(y_train.shape[1]))  \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train LSTM & GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM and GRU models are trained using Kubeflow, and the metrics (MAE, MSE) are logged. \n",
    "Training is executed via two scripts:\n",
    "- `train_LSTM.py` for LSTM model training.\n",
    "- `train_GRU.py` for GRU model training.\n",
    "\n",
    "The Kubeflow pipeline executes these scripts and tracks the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step: 7: Results from MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report saved as model_comparison_report.pdf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from fpdf import FPDF\n",
    "\n",
    "# Model metrics data\n",
    "models = ['LSTM', 'GRU']\n",
    "mae = [0.04076196786669234, 0.044035248215287405]\n",
    "mse = [0.009582408772798506, 0.010413049428058124]\n",
    "r2 = [0.9099495235355342, 0.9021435961794378]\n",
    "\n",
    "# Function to add data labels\n",
    "def add_data_labels(bars):\n",
    "    for bar in bars:\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height(),\n",
    "            f'{bar.get_height():.4f}',\n",
    "            ha='center',\n",
    "            va='bottom'\n",
    "        )\n",
    "\n",
    "# Plotting MAE comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(models, mae)\n",
    "add_data_labels(bars)\n",
    "plt.title('Model Comparison: Mean Absolute Error (MAE)')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('MAE')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('mae_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Plotting MSE comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(models, mse)\n",
    "add_data_labels(bars)\n",
    "plt.title('Model Comparison: Mean Squared Error (MSE)')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('MSE')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('mse_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Plotting R² comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(models, r2)\n",
    "add_data_labels(bars)\n",
    "plt.title('Model Comparison: R² Score')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('R² Score')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('r2_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Creating the PDF report\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "pdf.cell(200, 10, txt=\"Model Comparison Report\", ln=True, align=\"C\")\n",
    "pdf.cell(200, 10, txt=\"LSTM vs GRU Model Performance\", ln=True, align=\"C\")\n",
    "\n",
    "# Adding MAE visualization to PDF\n",
    "pdf.image('mae_comparison.png', x=10, y=30, w=180)\n",
    "pdf.add_page()\n",
    "\n",
    "# Adding MSE visualization to PDF\n",
    "pdf.image('mse_comparison.png', x=10, y=10, w=180)\n",
    "pdf.add_page()\n",
    "\n",
    "# Adding R² visualization to PDF\n",
    "pdf.image('r2_comparison.png', x=10, y=10, w=180)\n",
    "\n",
    "# Save the report as PDF\n",
    "pdf_file_path = 'model_comparison_report.pdf'\n",
    "pdf.output(pdf_file_path)\n",
    "\n",
    "print(f\"PDF report saved as {pdf_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postman Test: Output JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully extracted and saved to extracted_data.json.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File path to the HDF5 data\n",
    "file_path = r'C:\\foai\\94879-starter-code-Team-Project\\metr-la.h5'\n",
    "\n",
    "# Load the data using h5py\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    # Access the 'df' group and extract the 'block0_values' dataset\n",
    "    df_group = file['df']\n",
    "    data = df_group['block0_values'][:]\n",
    "\n",
    "# Ensure that data has the necessary structure\n",
    "# Extract the first 100 rows with 207 features\n",
    "extracted_data = data[:100, :207]\n",
    "\n",
    "# Convert the data into a format suitable for JSON\n",
    "json_ready_data = {\n",
    "    \"input\": extracted_data.tolist()\n",
    "}\n",
    "\n",
    "# Save the JSON data to a file for easy use in Postman\n",
    "output_json_path = 'extracted_data.json'\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(json_ready_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Data successfully extracted and saved to {output_json_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: LSTM\n",
    "##### Currently set in _1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16318\\anaconda3\\envs\\opAI_VM\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m686/686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - loss: 0.0429 - mean_absolute_error: 0.1204 - val_loss: 0.0144 - val_mean_absolute_error: 0.0686\n",
      "Epoch 2/5\n",
      "\u001b[1m686/686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - loss: 0.0132 - mean_absolute_error: 0.0621 - val_loss: 0.0123 - val_mean_absolute_error: 0.0526\n",
      "Epoch 3/5\n",
      "\u001b[1m686/686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - loss: 0.0110 - mean_absolute_error: 0.0532 - val_loss: 0.0107 - val_mean_absolute_error: 0.0605\n",
      "Epoch 4/5\n",
      "\u001b[1m686/686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - loss: 0.0101 - mean_absolute_error: 0.0502 - val_loss: 0.0098 - val_mean_absolute_error: 0.0580\n",
      "Epoch 5/5\n",
      "\u001b[1m686/686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 35ms/step - loss: 0.0100 - mean_absolute_error: 0.0507 - val_loss: 0.0101 - val_mean_absolute_error: 0.0515\n",
      "\u001b[1m857/857\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/10 16:03:24 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "2024/10/10 16:03:39 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense\n",
    "import logging\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Set up logging to a file (train.log)\n",
    "logging.basicConfig(filename='train.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Step 2: Load and preprocess the dataset\n",
    "def load_data():\n",
    "    with h5py.File(r'C:\\foai\\94879-starter-code-Team-Project\\metr-la.h5', 'r') as f:\n",
    "        df_group = f['df']\n",
    "        \n",
    "        # Load the column names (features) from 'block0_items'\n",
    "        columns = list(df_group['block0_items'][:].astype(str))\n",
    "        \n",
    "        # Load the actual data from 'block0_values'\n",
    "        data = df_group['block0_values'][:]\n",
    "        \n",
    "        # Convert the data into a DataFrame\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        \n",
    "        return df\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Use MinMaxScaler to normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    return data_scaled, scaler\n",
    "\n",
    "# Step 3: Create sequences for time-series forecasting with 12 time steps for both training and prediction\n",
    "def create_sequences(data, time_steps=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])  # Sequence of features for input\n",
    "        y.append(data[i + time_steps, 0])  # Predict the next value for the first feature\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Step 4: Define the LSTM and GRU models with 12 units in the output layer for predicting 12 steps\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1))  # Single unit as we predict only the next time step\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "    return model\n",
    "\n",
    "def build_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(64, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(GRU(32))\n",
    "    model.add(Dense(1))  # Single unit as we predict only the next time step\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "    return model\n",
    "\n",
    "# Step 5: Train and evaluate the model, and log to MLflow\n",
    "def train_and_evaluate(model_type=\"LSTM\"):\n",
    "    # Load and preprocess the data\n",
    "    data = load_data()\n",
    "    data_scaled, scaler = preprocess_data(data)\n",
    "\n",
    "    # Create sequences with 12 time steps for both training and prediction\n",
    "    X, y = create_sequences(data_scaled, time_steps=12)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Check the number of features in the dataset\n",
    "    num_features = X_train.shape[2] if len(X_train.shape) > 2 else 1\n",
    "\n",
    "    # Reshape the data to (samples, time_steps, num_features)\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], num_features))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], num_features))\n",
    "    \n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run():\n",
    "        # Select and build the model\n",
    "        if model_type == \"LSTM\":\n",
    "            model = build_lstm_model(input_shape)\n",
    "            logging.info(f\"Building LSTM model with input shape {input_shape}\")\n",
    "        elif model_type == \"GRU\":\n",
    "            model = build_gru_model(input_shape)\n",
    "            logging.info(f\"Building GRU model with input shape {input_shape}\")\n",
    "\n",
    "        # Log the model type as a parameter\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        logging.info(f\"Starting training for {model_type} model...\")\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "        # Make predictions on training data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "\n",
    "        # Make predictions on test data\n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics for test data\n",
    "        mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "        mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "        r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"mae_test\", mae_test)\n",
    "        mlflow.log_metric(\"mse_test\", mse_test)\n",
    "        mlflow.log_metric(\"r2_score_test\", r2_test)\n",
    "\n",
    "        logging.info(f\"Test metrics - MAE: {mae_test}, MSE: {mse_test}, R2: {r2_test}\")\n",
    "\n",
    "        # Save training and test data, including predictions\n",
    "        np.save('X_train.npy', X_train)\n",
    "        np.save('y_train.npy', y_train)\n",
    "        np.save('y_pred_train.npy', y_pred_train)  # Save training predictions\n",
    "        np.save('X_test.npy', X_test)\n",
    "        np.save('y_test.npy', y_test)\n",
    "        np.save(f'y_pred_test_{model_type}.npy', y_pred_test)  # Save test predictions\n",
    "\n",
    "        # Log the trained model to MLflow\n",
    "        mlflow.keras.log_model(model, \"traffic_prediction_model\")\n",
    "\n",
    "        # Save the model locally as 'lstm_model.h5'\n",
    "        model.save(f'{model_type.lower()}_model.h5')\n",
    "\n",
    "        # Upload the train.log file as an artifact\n",
    "        mlflow.log_artifact(\"train.log\")\n",
    "\n",
    "        logging.info(f\"{model_type} Model training complete. Test MAE: {mae_test}, MSE: {mse_test}, R2 Score: {r2_test}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example of running the training for LSTM\n",
    "    train_and_evaluate(model_type=\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: GRU\n",
    "##### For this to work in _1.py you need to swith LSTM with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16318\\anaconda3\\envs\\opAI_VM\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m686/686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 31ms/step - loss: 0.0608 - mean_absolute_error: 0.1424 - val_loss: 0.0110 - val_mean_absolute_error: 0.0532\n",
      "Epoch 2/5\n",
      "\u001b[1m686/686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - loss: 0.0114 - mean_absolute_error: 0.0580 - val_loss: 0.0104 - val_mean_absolute_error: 0.0502\n",
      "Epoch 3/5\n",
      "\u001b[1m686/686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - loss: 0.0099 - mean_absolute_error: 0.0526 - val_loss: 0.0093 - val_mean_absolute_error: 0.0451\n",
      "Epoch 4/5\n",
      "\u001b[1m686/686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - loss: 0.0099 - mean_absolute_error: 0.0505 - val_loss: 0.0101 - val_mean_absolute_error: 0.0582\n",
      "Epoch 5/5\n",
      "\u001b[1m686/686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - loss: 0.0099 - mean_absolute_error: 0.0496 - val_loss: 0.0093 - val_mean_absolute_error: 0.0425\n",
      "\u001b[1m857/857\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/10 19:18:23 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "2024/10/10 19:18:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense\n",
    "import logging\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Set up logging to a file (train.log)\n",
    "logging.basicConfig(filename='train.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Step 2: Load and preprocess the dataset\n",
    "def load_data():\n",
    "    with h5py.File(r'C:\\foai\\94879-starter-code-Team-Project\\metr-la.h5', 'r') as f:\n",
    "        df_group = f['df']\n",
    "        \n",
    "        # Load the column names (features) from 'block0_items'\n",
    "        columns = list(df_group['block0_items'][:].astype(str))\n",
    "        \n",
    "        # Load the actual data from 'block0_values'\n",
    "        data = df_group['block0_values'][:]\n",
    "        \n",
    "        # Convert the data into a DataFrame\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        \n",
    "        return df\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Use MinMaxScaler to normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    return data_scaled, scaler\n",
    "\n",
    "# Step 3: Create sequences for time-series forecasting with 12 time steps for both training and prediction\n",
    "def create_sequences(data, time_steps=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])  # Sequence of features for input\n",
    "        y.append(data[i + time_steps, 0])  # Predict the next value for the first feature\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Step 4: Define the LSTM and GRU models with 12 units in the output layer for predicting 12 steps\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1))  # Single unit as we predict only the next time step\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "    return model\n",
    "\n",
    "def build_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(64, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(GRU(32))\n",
    "    model.add(Dense(1))  # Single unit as we predict only the next time step\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "    return model\n",
    "\n",
    "# Step 5: Train and evaluate the model, and log to MLflow\n",
    "def train_and_evaluate(model_type=\"GRU\"):\n",
    "    # Load and preprocess the data\n",
    "    data = load_data()\n",
    "    data_scaled, scaler = preprocess_data(data)\n",
    "\n",
    "    # Create sequences with 12 time steps for both training and prediction\n",
    "    X, y = create_sequences(data_scaled, time_steps=12)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Check the number of features in the dataset\n",
    "    num_features = X_train.shape[2] if len(X_train.shape) > 2 else 1\n",
    "\n",
    "    # Reshape the data to (samples, time_steps, num_features)\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], num_features))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], num_features))\n",
    "    \n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run():\n",
    "        # Select and build the model\n",
    "        if model_type == \"LSTM\":\n",
    "            model = build_lstm_model(input_shape)\n",
    "            logging.info(f\"Building LSTM model with input shape {input_shape}\")\n",
    "        elif model_type == \"GRU\":\n",
    "            model = build_gru_model(input_shape)\n",
    "            logging.info(f\"Building GRU model with input shape {input_shape}\")\n",
    "\n",
    "        # Log the model type as a parameter\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        logging.info(f\"Starting training for {model_type} model...\")\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "        # Make predictions on training data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "\n",
    "        # Make predictions on test data\n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics for test data\n",
    "        mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "        mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "        r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"mae_test\", mae_test)\n",
    "        mlflow.log_metric(\"mse_test\", mse_test)\n",
    "        mlflow.log_metric(\"r2_score_test\", r2_test)\n",
    "\n",
    "        logging.info(f\"Test metrics - MAE: {mae_test}, MSE: {mse_test}, R2: {r2_test}\")\n",
    "\n",
    "        # Save training and test data, including predictions\n",
    "        np.save('X_train.npy', X_train)\n",
    "        np.save('y_train.npy', y_train)\n",
    "        np.save(f'y_pred_train_{model_type}.npy', y_pred_train)  # Save training predictions\n",
    "        np.save('X_test.npy', X_test)\n",
    "        np.save('y_test.npy', y_test)\n",
    "        np.save(f'y_pred_test_{model_type}.npy', y_pred_test)  # Save test predictions\n",
    "\n",
    "        # Log the trained model to MLflow\n",
    "        mlflow.keras.log_model(model, \"traffic_prediction_model\")\n",
    "\n",
    "        # Save the model locally as 'lstm_model.h5'\n",
    "        model.save(f'{model_type.lower()}_model.h5')\n",
    "\n",
    "        # Upload the train.log file as an artifact\n",
    "        mlflow.log_artifact(\"train.log\")\n",
    "\n",
    "        logging.info(f\"{model_type} Model training complete. Test MAE: {mae_test}, MSE: {mse_test}, R2 Score: {r2_test}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example of running the training for LSTM\n",
    "    train_and_evaluate(model_type=\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opAI_VM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
